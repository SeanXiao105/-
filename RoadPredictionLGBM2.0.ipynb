{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4526b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import lightgbm as lgb\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"  #不然会崩内核\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04322398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip --default-timeout=1000 install --index-url https://mirrors.aliyun.com/pypi/simple tensorflow-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "425cf487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#取出某一个时间片的特征（速度，eta速度，状态，车辆数）\n",
    "def get_base_info(x):\n",
    "    return [i.split(':')[-1] for i in x.split(' ')]\n",
    "#取出速度\n",
    "def get_speed(x):\n",
    "    return np.array([i.split(',')[0] for i in x], dtype='float16')\n",
    "#取出eta速度\n",
    "def get_eta(x):\n",
    "    return np.array([i.split(',')[1] for i in x], dtype='float16')\n",
    "#取出状态（畅通，缓行，拥堵）1,2,3,4 ----3,4官方说可以看成一个 hhh\n",
    "def get_state(x):\n",
    "    return [int(i.split(',')[2]) for i in x]\n",
    "#取出通过的车辆数量\n",
    "def get_cnt(x):\n",
    "    return np.array([i.split(',')[3] for i in x], dtype='int16')\n",
    "#对训练集或测试集进行处理\n",
    "def gen_feats(path, mode='is_train'):\n",
    "    df = pd.read_csv(path, sep=';', header=None)#sep分隔符，以；分割\n",
    "    df['link'] = df[0].apply(lambda x: x.split(' ')[0])\n",
    "    if mode == 'is_train':\n",
    "        df['label'] = df[0].apply(lambda x: int(x.split(' ')[1]))\n",
    "        df['label'] = df['label'].apply(lambda x: 3 if x > 3 else x)\n",
    "        df['label'] -= 1#标签从1，2，3变成0，1，2\n",
    "        df['current_slice_id'] = df[0].apply(lambda x: int(x.split(' ')[2]))\n",
    "        df['future_slice_id'] = df[0].apply(lambda x: int(x.split(' ')[3]))\n",
    "    else:\n",
    "        df['label'] = -1\n",
    "        df['current_slice_id'] = df[0].apply(lambda x: int(x.split(' ')[2]))\n",
    "        df['future_slice_id'] = df[0].apply(lambda x: int(x.split(' ')[3]))\n",
    "\n",
    "    df['time_diff'] = df['future_slice_id'] - df['current_slice_id']\n",
    "    df['curr_state'] = df[1].apply(lambda x: x.split(' ')[-1].split(':')[-1])#当前时间片的特征\n",
    "    df['curr_speed'] = df['curr_state'].apply(lambda x: x.split(',')[0])\n",
    "    df['curr_eta'] = df['curr_state'].apply(lambda x: x.split(',')[1])\n",
    "    df['curr_cnt'] = df['curr_state'].apply(lambda x: x.split(',')[3])\n",
    "    df['curr_state'] = df['curr_state'].apply(lambda x: x.split(',')[2])#当前时间片的状态label\n",
    "    del df[0]\n",
    "\n",
    "    for i in tqdm(range(1, 6)):#tqdm 显示一个加载进度条\n",
    "        df['his_info'] = df[i].apply(get_base_info)\n",
    "        if i == 1:\n",
    "            flg = 'current'\n",
    "        else:\n",
    "            flg = f'his_{(6 - i) * 7}'\n",
    "        #提取每一段时间片的数据特征\n",
    "        df['his_speed'] = df['his_info'].apply(get_speed)\n",
    "        df[f'{flg}_speed_min'] = df['his_speed'].apply(lambda x: x.min())\n",
    "        df[f'{flg}_speed_max'] = df['his_speed'].apply(lambda x: x.max())\n",
    "        df[f'{flg}_speed_mean'] = df['his_speed'].apply(lambda x: x.mean())\n",
    "        df[f'{flg}_speed_std'] = df['his_speed'].apply(lambda x: x.std())\n",
    "\n",
    "        df['his_eta'] = df['his_info'].apply(get_eta)\n",
    "        df[f'{flg}_eta_min'] = df['his_eta'].apply(lambda x: x.min())\n",
    "        df[f'{flg}_eta_max'] = df['his_eta'].apply(lambda x: x.max())\n",
    "        df[f'{flg}_eta_mean'] = df['his_eta'].apply(lambda x: x.mean())\n",
    "        df[f'{flg}_eta_std'] = df['his_eta'].apply(lambda x: x.std())\n",
    "\n",
    "        df['his_cnt'] = df['his_info'].apply(get_cnt)\n",
    "        df[f'{flg}_cnt_min'] = df['his_cnt'].apply(lambda x: x.min())\n",
    "        df[f'{flg}_cnt_max'] = df['his_cnt'].apply(lambda x: x.max())\n",
    "        df[f'{flg}_cnt_mean'] = df['his_cnt'].apply(lambda x: x.mean())\n",
    "        df[f'{flg}_cnt_std'] = df['his_cnt'].apply(lambda x: x.std())\n",
    "\n",
    "        df['his_state'] = df['his_info'].apply(get_state)\n",
    "        #counter（）函数返回的是一个类似于字典的counter计数器\n",
    "        #Counter类中的most_common(n)函数:传进去一个可选参数n(代表获取数量最多的前n个元素，如果不传参数，代表返回所有结果)\n",
    "        df[f'{flg}_state'] = df['his_state'].apply(lambda x: Counter(x).most_common()[0][0])\n",
    "        df.drop([i, 'his_info', 'his_speed', 'his_eta', 'his_cnt', 'his_state'], axis=1, inplace=True)\n",
    "    if mode == 'is_train':\n",
    "        r='\\\\'\n",
    "        df.to_csv(f\"__{mode}_{path.split(r)[-1]}\", index=False)\n",
    "    else:\n",
    "        df.to_csv(f\"is_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a97e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#算法评价指标\n",
    "def f1_score_eval(preds, valid_df):\n",
    "    labels = valid_df.get_label()\n",
    "    preds = np.argmax(preds.reshape(3, -1), axis=0)\n",
    "    scores = f1_score(y_true=labels, y_pred=preds, average=None)\n",
    "    scores = scores[0]*0.2+scores[1]*0.2+scores[2]*0.6\n",
    "    return 'f1_score', scores, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4185fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen_feats(r\"D:\\RoadStatusData\\traffic\\20190701.txt\", mode='is_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5fea763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_train(train_: pd.DataFrame, test_: pd.DataFrame, use_train_feats: list, id_col: str, label: str,\n",
    "              n_splits: int, split_rs: int, is_shuffle=True, use_cart=False, cate_cols=None) -> pd.DataFrame:\n",
    "    if not cate_cols:\n",
    "        cate_cols = []\n",
    "    print('data shape:\\ntrain--{}\\ntest--{}'.format(train_.shape, test_.shape))#数据维度\n",
    "    print('Use {} features ...'.format(len(use_train_feats)))#有几个特征\n",
    "    print('Use lightgbm to train ...')\n",
    "    n_class = train_[label].nunique()#unique()是以 数组形式（numpy.ndarray）返回列的所有唯一值（特征的所有唯一值）\n",
    "                                     #nunique() 返回的是唯一值的个数\n",
    "                                     #这里n_class代表有几个分类，1，2，3即三种\n",
    "    train_[f'{label}_pred'] = 0\n",
    "    test_pred = np.zeros((test_.shape[0], n_class))#预测先置为0\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = use_train_feats\n",
    "\n",
    "    folds = KFold(n_splits=n_splits, shuffle=is_shuffle, random_state=split_rs)#n_split:要划分的折数\n",
    "                                                                               #shuffle: 每次都进行shuffle，测试集中折数的总和就是训练集的个数\n",
    "                                                                               #random_state:随机状态\n",
    "    train_user_id = train_[id_col].unique()#返回所有id的唯一值\n",
    "\n",
    "    #模型参数设置\n",
    "    params = {\n",
    "        'learning_rate': 0.05,#学习率\n",
    "        'boosting_type': 'gbdt', #gbdt模型为基础\n",
    "        'objective': 'multiclass',#多分类\n",
    "        'metric': 'None',\n",
    "        'num_leaves': 31,#单棵树的最大叶子数\n",
    "        'num_class': n_class,#共有多少类\n",
    "        'feature_fraction': 0.8,# 如果 feature_fraction 小于 1.0, LightGBM 将会在每次迭代中随机选择部分特征. 例如, 如果设置为 0.8, 将会在每棵树训练之前选择 80% 的特征\n",
    "                                # 可以用来加速训练\n",
    "                                # 可以用来处理过拟合\n",
    "        'bagging_fraction': 0.8,# 类似于 feature_fraction, 但是它将在不进行重采样的情况下随机选择部分数据\n",
    "                                # 可以用来加速训练\n",
    "                                # 可以用来处理过拟合\n",
    "                                # Note: 为了启用 bagging, bagging_freq 应该设置为非零值\n",
    "        'bagging_freq': 5,#bagging 的频率, 0 意味着禁用 bagging. k 意味着每 k 次迭代执行bagging\n",
    "                          #Note: 为了启用 bagging, bagging_fraction 设置适当\n",
    "        'seed': 1,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction_seed': 7,\n",
    "        'min_data_in_leaf': 20,#一个叶子上数据的最小数量. 可以用来处理过拟合\n",
    "        'nthread': -1,\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_user_id), start=1):#把数据分成几折\n",
    "        #分成训练集和测试集的下标\n",
    "        print('the {} training start ...'.format(n_fold))\n",
    "        train_x, train_y = train_.loc[train_[id_col].isin(train_user_id[train_idx]), use_train_feats], train_.loc[\n",
    "            train_[id_col].isin(train_user_id[train_idx]), label]#bool索引  拆分成特征和标签\n",
    "        valid_x, valid_y = train_.loc[train_[id_col].isin(train_user_id[valid_idx]), use_train_feats], train_.loc[\n",
    "            train_[id_col].isin(train_user_id[valid_idx]), label]#被分成测试集的特征和标签\n",
    "        print(f'for train user:{len(train_idx)}\\nfor valid user:{len(valid_idx)}')\n",
    "\n",
    "        if use_cart:\n",
    "            dtrain = lgb.Dataset(train_x, label=train_y, categorical_feature=cate_cols)\n",
    "            #lightgbm可以处理标称型（类别）数据。通过指定'categorical_feature' 这一参数告诉它哪些feature是标称型的。\n",
    "            # 它不需要将数据展开成独热码(one-hot)，其原理是对特征的所有取值，做一个one-vs-others，从而找出最佳分割的那一个特征取值\n",
    "            dvalid = lgb.Dataset(valid_x, label=valid_y, categorical_feature=cate_cols)\n",
    "        else:\n",
    "            dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "            dvalid = lgb.Dataset(valid_x, label=valid_y)\n",
    "            \n",
    "        #训练\n",
    "        clf = lgb.train(\n",
    "            params=params,#模型参数\n",
    "            train_set=dtrain,#训练集\n",
    "            num_boost_round=5000,#迭代次数\n",
    "            valid_sets=[dvalid],#测试集\n",
    "            early_stopping_rounds=100,#如果一次验证数据的一个度量在最近的early_stopping_round 回合中没有提高，模型将停止训练 加速分析，减少过多迭代\n",
    "            verbose_eval=100,#迭代几次传回评估结果---正确率召回率指标\n",
    "            feval=f1_score_eval#评价函数\n",
    "        )\n",
    "         #  统计某种特征在整个.py文件中使用的次数\n",
    "        fold_importance_df[f'fold_{n_fold}_imp'] = clf.feature_importance(importance_type='gain')#特征重要度 哈哈哈\n",
    "        train_.loc[train_[id_col].isin(train_user_id[valid_idx]), f'{label}_pred'] = np.argmax(\n",
    "            clf.predict(valid_x, num_iteration=clf.best_iteration), axis=1)\n",
    "        test_pred += clf.predict(test_[use_train_feats], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "    report = f1_score(train_[label], train_[f'{label}_pred'], average=None)#计算分数\n",
    "    print(classification_report(train_[label], train_[f'{label}_pred'], digits=4))#评估指标 4个小数\n",
    "    print('Score: ', report[0] * 0.2 + report[1] * 0.2 + report[2] * 0.6)\n",
    "    test_[f'{label}_pred'] = np.argmax(test_pred, axis=1)\n",
    "    test_[label] = np.argmax(test_pred, axis=1)+1 #测试集标签\n",
    "    \n",
    "    # test_[label] = np.argmax(test_pred, axis=1)\n",
    "    #统计数据\n",
    "    five_folds = [f'fold_{f}_imp' for f in range(1, n_splits + 1)]\n",
    "    fold_importance_df['avg_imp'] = fold_importance_df[five_folds].mean(axis=1)\n",
    "    fold_importance_df.sort_values(by='avg_imp', ascending=False, inplace=True)\n",
    "    print(fold_importance_df[['Feature', 'avg_imp']].head(20))\n",
    "    return test_[[id_col, 'current_slice_id', 'future_slice_id', label]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b9ba1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for i in tqdm(range(25,31)):\\n    with open(f'D:\\\\RoadStatusData\\\\traffic\\\\201907{i}.txt', 'r') as text:\\n      with open(r'D:\\\\RoadStatusData\\traffic\\\\merge_25_30.txt', 'a') as txt:\\n        txt.writelines(text.readlines())\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将后6天的训练集合并  失败   \n",
    "'''\n",
    "df1=pd.read_csv(r\"D:\\RoadStatusData\\traffic\\20190725.txt\")\n",
    "for i in range(26,31):\n",
    "    df2=pd.read_csv(f\"D:\\\\RoadStatusData\\\\traffic\\\\201907{i}.txt\")\n",
    "    df1=pd.concat([df1,df2])\n",
    "df1.to_csv(r\"merge_25_30.txt\", index=False, encoding='utf8')'''\n",
    "#采用写入的方式    成功\n",
    "'''for i in tqdm(range(25,31)):\n",
    "    with open(f'D:\\\\RoadStatusData\\\\traffic\\\\201907{i}.txt', 'r') as text:\n",
    "      with open(r'D:\\RoadStatusData\\traffic\\merge_25_30.txt', 'a') as txt:\n",
    "        txt.writelines(text.readlines())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eee12d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'topo=pd.read_csv(r\"D:\\\\RoadStatusData\\\\topo.txt\",sep=\\'\\t\\',header=None)\\ntopo[\"link\"]=topo[0]\\ntopo[\"counts\"]=topo[1].apply(lambda x:len(x.split(\\',\\')))\\ndel topo[0]\\ndel topo[1]\\ntopo\\ntopo.to_csv(r\"topo.csv\", index=False, encoding=\\'utf8\\')'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#看看加上topo的下流数量 效果怎么样\n",
    "'''topo=pd.read_csv(r\"D:\\\\RoadStatusData\\\\topo.txt\",sep='\\t',header=None)\n",
    "topo[\"link\"]=topo[0]\n",
    "topo[\"counts\"]=topo[1].apply(lambda x:len(x.split(',')))\n",
    "del topo[0]\n",
    "del topo[1]\n",
    "topo\n",
    "topo.to_csv(r\"topo.csv\", index=False, encoding='utf8')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc13b62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'topo=pd.read_csv(r\"topo.csv\")\\nattr = pd.read_csv(r\\'D:\\\\RoadStatusData\\x07ttr.txt\\', sep=\\'\\t\\',\\n                       names=[\\'link\\', \\'length\\', \\'direction\\', \\'path_class\\', \\'speed_class\\', \\'LaneNum\\', \\'speed_limit\\',\\n                              \\'level\\', \\'width\\'], header=None)\\nattr=attr.merge(topo,on=\\'link\\',how=\\'left\\')\\nattr.to_csv(r\"attr_topo.csv\", index=False, encoding=\\'utf8\\')'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将topo和attr表和在一起\n",
    "'''topo=pd.read_csv(r\"topo.csv\")\n",
    "attr = pd.read_csv(r'D:\\RoadStatusData\\attr.txt', sep='\\t',\n",
    "                       names=['link', 'length', 'direction', 'path_class', 'speed_class', 'LaneNum', 'speed_limit',\n",
    "                              'level', 'width'], header=None)\n",
    "attr=attr.merge(topo,on='link',how='left')\n",
    "attr.to_csv(r\"attr_topo.csv\", index=False, encoding='utf8')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b113ea87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:\n",
      "train--(2989065, 83)\n",
      "test--(176057, 83)\n",
      "Use 79 features ...\n",
      "Use lightgbm to train ...\n",
      "the 1 training start ...\n",
      "for train user:11276\n",
      "for valid user:2819\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's f1_score: 0.588456\n",
      "[200]\tvalid_0's f1_score: 0.591883\n",
      "[300]\tvalid_0's f1_score: 0.592455\n",
      "[400]\tvalid_0's f1_score: 0.592438\n",
      "[500]\tvalid_0's f1_score: 0.592767\n",
      "Early stopping, best iteration is:\n",
      "[470]\tvalid_0's f1_score: 0.593387\n",
      "the 2 training start ...\n",
      "for train user:11276\n",
      "for valid user:2819\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's f1_score: 0.58912\n",
      "[200]\tvalid_0's f1_score: 0.591581\n",
      "[300]\tvalid_0's f1_score: 0.592435\n",
      "[400]\tvalid_0's f1_score: 0.592214\n",
      "Early stopping, best iteration is:\n",
      "[370]\tvalid_0's f1_score: 0.592527\n",
      "the 3 training start ...\n",
      "for train user:11276\n",
      "for valid user:2819\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's f1_score: 0.603732\n",
      "[200]\tvalid_0's f1_score: 0.608259\n",
      "[300]\tvalid_0's f1_score: 0.609457\n",
      "[400]\tvalid_0's f1_score: 0.609748\n",
      "Early stopping, best iteration is:\n",
      "[380]\tvalid_0's f1_score: 0.610061\n",
      "the 4 training start ...\n",
      "for train user:11276\n",
      "for valid user:2819\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's f1_score: 0.590137\n",
      "[200]\tvalid_0's f1_score: 0.592975\n",
      "[300]\tvalid_0's f1_score: 0.593482\n",
      "[400]\tvalid_0's f1_score: 0.594202\n",
      "[500]\tvalid_0's f1_score: 0.594864\n",
      "[600]\tvalid_0's f1_score: 0.594806\n",
      "[700]\tvalid_0's f1_score: 0.595452\n",
      "[800]\tvalid_0's f1_score: 0.595894\n",
      "[900]\tvalid_0's f1_score: 0.596539\n",
      "[1000]\tvalid_0's f1_score: 0.596717\n",
      "Early stopping, best iteration is:\n",
      "[987]\tvalid_0's f1_score: 0.596915\n",
      "the 5 training start ...\n",
      "for train user:11276\n",
      "for valid user:2819\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's f1_score: 0.610628\n",
      "[200]\tvalid_0's f1_score: 0.615464\n",
      "[300]\tvalid_0's f1_score: 0.616202\n",
      "[400]\tvalid_0's f1_score: 0.617194\n",
      "[500]\tvalid_0's f1_score: 0.616753\n",
      "[600]\tvalid_0's f1_score: 0.617278\n",
      "Early stopping, best iteration is:\n",
      "[562]\tvalid_0's f1_score: 0.617564\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9047    0.9606    0.9318   2430483\n",
      "           1     0.6299    0.4719    0.5396    430653\n",
      "           2     0.6400    0.4289    0.5136    127929\n",
      "\n",
      "    accuracy                         0.8674   2989065\n",
      "   macro avg     0.7249    0.6205    0.6617   2989065\n",
      "weighted avg     0.8538    0.8674    0.8574   2989065\n",
      "\n",
      "Score:  0.602443401836487\n",
      "               Feature       avg_imp\n",
      "69         his_7_state  2.271002e+06\n",
      "1           curr_state  1.186739e+06\n",
      "17       current_state  9.670854e+05\n",
      "5    current_speed_min  8.892536e+05\n",
      "57     his_7_speed_min  8.355622e+05\n",
      "56        his_14_state  7.700476e+05\n",
      "44    his_14_speed_min  5.316426e+05\n",
      "43        his_21_state  3.503183e+05\n",
      "31    his_21_speed_min  3.363607e+05\n",
      "30        his_28_state  2.085854e+05\n",
      "18    his_28_speed_min  1.345801e+05\n",
      "7   current_speed_mean  1.260152e+05\n",
      "0            time_diff  1.096515e+05\n",
      "59    his_7_speed_mean  6.792030e+04\n",
      "58     his_7_speed_max  6.692761e+04\n",
      "6    current_speed_max  6.262594e+04\n",
      "46   his_14_speed_mean  6.057891e+04\n",
      "33   his_21_speed_mean  5.431518e+04\n",
      "45    his_14_speed_max  5.320693e+04\n",
      "74             LaneNum  5.198489e+04\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #train_path = r'D:\\RoadStatusData\\traffic\\20190730.txt'\n",
    "    '''train_path=r'D:\\RoadStatusData\\traffic\\merge_25_30.txt'\n",
    "    test_path = r'D:\\RoadStatusData\\20190801_testdata.txt'\n",
    "    gen_feats(train_path, mode='is_train')\n",
    "    gen_feats(test_path, mode='is_test')'''\n",
    "    '''attr = pd.read_csv(r'D:\\RoadStatusData\\attr.txt', sep='\\t',\n",
    "                       names=['link', 'length', 'direction', 'path_class', 'speed_class', 'LaneNum', 'speed_limit',\n",
    "                              'level', 'width'], header=None)'''\n",
    "    attr=pd.read_csv(r\"attr_topo.csv\")\n",
    "\n",
    "    train = pd.read_csv('__is_train_merge_25_30.txt')\n",
    "    test = pd.read_csv('is_test.csv')\n",
    "    train = train.merge(attr, on='link', how='left')\n",
    "    test = test.merge(attr, on='link', how='left')\n",
    "\n",
    "    use_cols = [i for i in train.columns if i not in ['link', 'label', 'current_slice_id', 'future_slice_id', 'label_pred']]\n",
    "\n",
    "    sub = lgb_train(train, test, use_cols, 'link', 'label', 5, 2020)\n",
    "\n",
    "    sub.to_csv('RoadLGBMPre.csv', index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9aa3ecae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df=pd.read_csv(r'D:\\\\RoadStatusData\\traffic\\x8190701.txt', sep=';',header=None)\\ndf.head()\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df=pd.read_csv(r'D:\\RoadStatusData\\traffic\\20190701.txt', sep=';',header=None)\n",
    "df.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "325cc917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df['link'] = df[0].apply(lambda x: x.split(' ')[0])\\ndf['label'] = df[0].apply(lambda x: int(x.split(' ')[1]))\\ndf['label'] = df['label'].apply(lambda x: 3 if x > 3 else x)\\ndf['label'] -= 1\\ndf['current_slice_id'] = df[0].apply(lambda x: int(x.split(' ')[2]))\\ndf['future_slice_id'] = df[0].apply(lambda x: int(x.split(' ')[3]))\\n\\ndf['time_diff'] = df['future_slice_id'] - df['current_slice_id']\\ndf['curr_state'] = df[1].apply(lambda x: x.split(' ')[-1].split(':')[-1])#取当前时间片的特征\\ndf['curr_speed'] = df['curr_state'].apply(lambda x: x.split(',')[0])\\ndf['curr_eta'] = df['curr_state'].apply(lambda x: x.split(',')[1])\\ndf['curr_cnt'] = df['curr_state'].apply(lambda x: x.split(',')[3])\\ndf['curr_state'] = df['curr_state'].apply(lambda x: x.split(',')[2])#当前时间片的状态label\\ndel df[0]\\ndf\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df['link'] = df[0].apply(lambda x: x.split(' ')[0])\n",
    "df['label'] = df[0].apply(lambda x: int(x.split(' ')[1]))\n",
    "df['label'] = df['label'].apply(lambda x: 3 if x > 3 else x)\n",
    "df['label'] -= 1\n",
    "df['current_slice_id'] = df[0].apply(lambda x: int(x.split(' ')[2]))\n",
    "df['future_slice_id'] = df[0].apply(lambda x: int(x.split(' ')[3]))\n",
    "\n",
    "df['time_diff'] = df['future_slice_id'] - df['current_slice_id']\n",
    "df['curr_state'] = df[1].apply(lambda x: x.split(' ')[-1].split(':')[-1])#取当前时间片的特征\n",
    "df['curr_speed'] = df['curr_state'].apply(lambda x: x.split(',')[0])\n",
    "df['curr_eta'] = df['curr_state'].apply(lambda x: x.split(',')[1])\n",
    "df['curr_cnt'] = df['curr_state'].apply(lambda x: x.split(',')[3])\n",
    "df['curr_state'] = df['curr_state'].apply(lambda x: x.split(',')[2])#当前时间片的状态label\n",
    "del df[0]\n",
    "df'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
